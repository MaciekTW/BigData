22/06/12 15:07:26 INFO StaticConf$: DB_HOME: /databricks
22/06/12 15:07:28 INFO DriverDaemon$: Current JVM Version 1.8.0_302
22/06/12 15:07:29 INFO DriverDaemon$: ========== driver starting up ==========
22/06/12 15:07:29 INFO DriverDaemon$: Java: Azul Systems, Inc. 1.8.0_302
22/06/12 15:07:29 INFO DriverDaemon$: OS: Linux/amd64 5.4.0-1072-aws
22/06/12 15:07:29 INFO DriverDaemon$: CWD: /databricks/driver
22/06/12 15:07:29 INFO DriverDaemon$: Mem: Max: 7.3G loaded GCs: PS Scavenge, PS MarkSweep
22/06/12 15:07:29 INFO DriverDaemon$: Logging multibyte characters: âœ“
22/06/12 15:07:29 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
22/06/12 15:07:29 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
22/06/12 15:07:29 INFO DriverDaemon$: == Modules:
22/06/12 15:07:31 INFO DriverDaemon$: Starting prometheus metrics log export timer
22/06/12 15:07:31 INFO DriverDaemon$: Loaded JDBC drivers in 169 ms
22/06/12 15:07:31 INFO DriverDaemon$: Universe Git Hash: f04ea920e157163c2b26dbf8a226dd1480f2a18d
22/06/12 15:07:31 INFO DriverDaemon$: Spark Git Hash: c7e870a91cc69e90a4c4fb9696604a539309a4b0
22/06/12 15:07:31 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
22/06/12 15:07:31 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,false,false,List())
22/06/12 15:07:31 INFO DatabricksILoop$: Creating throwaway interpreter
22/06/12 15:07:31 WARN MetastoreMonitor$: Unexpected metastore configuration.
22/06/12 15:07:32 INFO MetastoreMonitor: Not monitoring UnknownMetastore
22/06/12 15:07:32 INFO DriverCorral: Creating the driver context
22/06/12 15:07:32 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-2593776914622800352-5aa7cc5b-9501-454f-9a88-327c8367d0d9
22/06/12 15:07:32 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
22/06/12 15:07:32 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
22/06/12 15:07:36 INFO SparkContext: Running Spark version 3.2.1
22/06/12 15:07:37 INFO ResourceUtils: ==============================================================
22/06/12 15:07:37 INFO ResourceUtils: No custom resources configured for spark.driver.
22/06/12 15:07:37 INFO ResourceUtils: ==============================================================
22/06/12 15:07:37 INFO SparkContext: Submitted application: Databricks Shell
22/06/12 15:07:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 8278, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/06/12 15:07:37 INFO ResourceProfile: Limiting resource is cpu
22/06/12 15:07:37 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/06/12 15:07:37 INFO SecurityManager: Changing view acls to: root
22/06/12 15:07:37 INFO SecurityManager: Changing modify acls to: root
22/06/12 15:07:37 INFO SecurityManager: Changing view acls groups to: 
22/06/12 15:07:37 INFO SecurityManager: Changing modify acls groups to: 
22/06/12 15:07:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
22/06/12 15:07:38 INFO Utils: Successfully started service 'sparkDriver' on port 37547.
22/06/12 15:07:38 INFO SparkEnv: Registering MapOutputTracker
22/06/12 15:07:38 INFO SparkEnv: Registering BlockManagerMaster
22/06/12 15:07:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/06/12 15:07:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/06/12 15:07:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/06/12 15:07:38 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-eb0fdac2-5785-4029-951a-b10ad2389bfd
22/06/12 15:07:38 INFO MemoryStore: MemoryStore started with capacity 3.9 GiB
22/06/12 15:07:38 INFO SparkEnv: Registering OutputCommitCoordinator
22/06/12 15:07:39 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600
libraryDownload.sleepIntervalSeconds=5
libraryDownload.timeoutSeconds=180
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.app.startTime=1655046456891
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient
spark.databricks.cloudProvider=AWS
spark.databricks.cloudfetch.hasRegionSupport=true
spark.databricks.cloudfetch.requesterClassName=*********(redacted)
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.attribute_tag_budget=
spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env=
spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer=
spark.databricks.clusterUsageTags.attribute_tag_dust_suite=
spark.databricks.clusterUsageTags.attribute_tag_service=
spark.databricks.clusterUsageTags.autoTerminationMinutes=120
spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus=false
spark.databricks.clusterUsageTags.cloudProvider=AWS
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Name","value":"ce-worker"}]
spark.databricks.clusterUsageTags.clusterAvailability=ON_DEMAND
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterEbsVolumeCount=0
spark.databricks.clusterUsageTags.clusterEbsVolumeSize=0
spark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD
spark.databricks.clusterUsageTags.clusterFirstOnDemand=0
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=0612-150705-bsgfzgx3
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterName=test
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=dev-tier-node
spark.databricks.clusterUsageTags.clusterNumCustomTags=0
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=2048622803436590
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=fixed_size
spark.databricks.clusterUsageTags.clusterSizeType=VM_CONTAINER
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=0
spark.databricks.clusterUsageTags.clusterUnityCatalogMode=CUSTOM
spark.databricks.clusterUsageTags.clusterWorkers=0
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.containerZoneId=us-west-2b
spark.databricks.clusterUsageTags.dataPlaneRegion=us-west-2
spark.databricks.clusterUsageTags.driverContainerId=20a07999300d42168b12f959ea82087f
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.172.203.52
spark.databricks.clusterUsageTags.driverInstanceId=i-087fe839f6c43fe13
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.172.195.207
spark.databricks.clusterUsageTags.driverNodeType=dev-tier-node
spark.databricks.clusterUsageTags.driverPublicDns=ec2-34-220-181-175.us-west-2.compute.amazonaws.com
spark.databricks.clusterUsageTags.effectiveSparkVersion=10.4.x-scala2.12
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=false
spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.instanceBootstrapType=ssh
spark.databricks.clusterUsageTags.instanceProfileUsed=false
spark.databricks.clusterUsageTags.instanceWorkerEnvId=default-worker-env
spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=default
spark.databricks.clusterUsageTags.isIMv2Enabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.ngrokNpipEnabled=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0
spark.databricks.clusterUsageTags.orgId=2048622803436590
spark.databricks.clusterUsageTags.privateLinkEnabled=false
spark.databricks.clusterUsageTags.region=us-west-2
spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline=false
spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes=false
spark.databricks.clusterUsageTags.sparkVersion=10.4.x-scala2.12
spark.databricks.clusterUsageTags.userId=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedSparkVersion=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=default-worker-env
spark.databricks.credential.aws.secretKey.redactor=*********(redacted)
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNfs.enabled=true
spark.databricks.driverNfs.pathSuffix=.ephemeral_nfs
spark.databricks.driverNodeTypeId=dev-tier-node
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.managedCatalog.clientClassName=com.databricks.managedcatalog.ManagedCatalogClientImpl
spark.databricks.managedCatalog.gcs.tokenProviderClassName=*********(redacted)
spark.databricks.managedCatalog.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.metrics.filesystem_io_metrics=true
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=*********(redacted)
spark.databricks.passthrough.oauth.refresher.impl=*********(redacted)
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.python.defaultPythonRepl=ipykernel
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.repl.enableClassFileCleanup=true
spark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils
spark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils
spark.databricks.session.share=false
spark.databricks.sparkContextId=2593776914622800352
spark.databricks.sql.configMapperClass=com.databricks.dbsql.config.SqlConfigMapperBridge
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.S3LockBasedLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore
spark.databricks.workerNodeTypeId=dev-tier-node
spark.databricks.workspace.matplotlibInline.enabled=true
spark.databricks.workspace.multipleResults.enabled=true
spark.delta.sharing.profile.provider.class=*********(redacted)
spark.driver.allowMultipleContexts=false
spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED
spark.driver.host=10.172.203.52
spark.driver.maxResultSize=4g
spark.driver.port=37547
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=8278m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v1
spark.hadoop.databricks.fs.perfMetrics.enable=true
spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.abfs.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.authorization.caching.enable=false
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.cpfs-abfss.impl=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl.disable.cache=true
spark.hadoop.fs.cpfs-adl.impl=*********(redacted)
spark.hadoop.fs.cpfs-adl.impl.disable.cache=true
spark.hadoop.fs.cpfs-s3.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3a.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3n.impl=*********(redacted)
spark.hadoop.fs.dbfs.impl=com.databricks.backend.daemon.data.client.DBFS
spark.hadoop.fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1
spark.hadoop.fs.elfs.impl=com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem
spark.hadoop.fs.elfs.impl.disable.cache=true
spark.hadoop.fs.fcfs-abfs.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfs.impl.disable.cache=true
spark.hadoop.fs.fcfs-abfss.impl=*********(redacted)
spark.hadoop.fs.fcfs-abfss.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3a.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3a.impl.disable.cache=true
spark.hadoop.fs.fcfs-s3n.impl=*********(redacted)
spark.hadoop.fs.fcfs-s3n.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasb.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasb.impl.disable.cache=true
spark.hadoop.fs.fcfs-wasbs.impl=*********(redacted)
spark.hadoop.fs.fcfs-wasbs.impl.disable.cache=true
spark.hadoop.fs.file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem
spark.hadoop.fs.gs.impl=shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.fs.gs.impl.disable.cache=true
spark.hadoop.fs.gs.outputstream.upload.chunk.size=16777216
spark.hadoop.fs.idbfs.impl=com.databricks.io.idbfs.IdbfsFileSystem
spark.hadoop.fs.mcfs-abfss.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-abfss.impl.disable.cache=true
spark.hadoop.fs.mcfs-gs.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-s3.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-s3a.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-s3n.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.s3.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.assumed.role.credentials.provider=*********(redacted)
spark.hadoop.fs.s3a.attempts.maximum=10
spark.hadoop.fs.s3a.block.size=67108864
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.connection.timeout=50000
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.active.blocks=32
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.max.total.tasks=1000
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.retry.limit=20
spark.hadoop.fs.s3a.retry.throttle.interval=500ms
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.hmshandler.retry.attempts=10
spark.hadoop.hive.hmshandler.retry.interval=2000
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.abfs.readahead.optimization.enabled=false
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.filter.columnindex.enabled=false
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.metadata.validation.enabled=true
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException=true
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.home=/databricks/spark
spark.logConf=true
spark.master=local[8]
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-2593776914622800352-5aa7cc5b-9501-454f-9a88-327c8367d0d9
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparklyr-backend.threads=1
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/databricks-hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.legacy.createHiveTableByDefault=false
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.default=delta
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.streaming.stopTimeout=15s
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=40001
spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient
spark.worker.aioaLazyConfig.iamReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosIamRoleCheckClient
spark.worker.cleanup.enabled=false
22/06/12 15:07:39 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
22/06/12 15:07:39 INFO log: Logging initialized @16129ms to org.eclipse.jetty.util.log.Slf4jLog
22/06/12 15:07:39 INFO Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_302-b08
22/06/12 15:07:39 INFO Server: Started @16603ms
22/06/12 15:07:40 INFO AbstractConnector: Started ServerConnector@1f71e024{HTTP/1.1, (http/1.1)}{10.172.203.52:40001}
22/06/12 15:07:40 INFO Utils: Successfully started service 'SparkUI' on port 40001.
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33215ffb{/jobs,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@110620d{/jobs/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@22aee519{/jobs/job,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@30af23fd{/jobs/job/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@a357c3e{/stages,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3eb9c575{/stages/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e477944{/stages/stage,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1716c037{/stages/stage/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@708769b7{/stages/pool,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@135e49b2{/stages/pool/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10b87ff6{/storage,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2d4f67e{/storage/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@41167ded{/storage/rdd,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36238b12{/storage/rdd/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@41938e1e{/environment,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77ff14ce{/environment/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5a8d676e{/executors,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7ba06506{/executors/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@525b8922{/executors/threadDump,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6caa4dc5{/executors/threadDump/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60c98f25{/executors/heapHistogram,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1eb906f3{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@74fc3fc7{/static,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4bbc21bd{/,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d85b399{/api,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55e91e61{/jobs/job/kill,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d55dd21{/stages/stage/kill,null,AVAILABLE,@Spark}
22/06/12 15:07:40 INFO SparkUI: Bound SparkUI to 10.172.203.52, and started at http://10.172.203.52:40001
22/06/12 15:07:40 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
22/06/12 15:07:40 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
22/06/12 15:07:40 INFO Executor: Starting executor ID driver on host ip-10-172-203-52.us-west-2.compute.internal
22/06/12 15:07:40 INFO Executor: Using REPL class URI: spark://10.172.203.52:37547/classes
22/06/12 15:07:40 INFO TaskSchedulerImpl: Task preemption enabled.
22/06/12 15:07:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38873.
22/06/12 15:07:40 INFO NettyBlockTransferService: Server created on 10.172.203.52:38873
22/06/12 15:07:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/06/12 15:07:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.172.203.52, 38873, None)
22/06/12 15:07:40 INFO BlockManagerMasterEndpoint: Registering block manager 10.172.203.52:38873 with 3.9 GiB RAM, BlockManagerId(driver, 10.172.203.52, 38873, None)
22/06/12 15:07:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.172.203.52, 38873, None)
22/06/12 15:07:41 INFO BlockManager: external shuffle service port = 4048
22/06/12 15:07:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.172.203.52, 38873, None)
22/06/12 15:07:41 INFO DatabricksILoop$: Finished creating throwaway interpreter
22/06/12 15:07:41 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@67c0c90b{/metrics/json,null,AVAILABLE,@Spark}
22/06/12 15:07:41 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
22/06/12 15:07:41 INFO DBCEventLoggingListener: Logging events to eventlogs/2593776914622800352/eventlog
22/06/12 15:07:41 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
22/06/12 15:07:41 INFO SparkContext: Loading Spark Service RPC Server. Classloader stack:List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@5ec9eefa, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@356fa0d1, sun.misc.Launcher$AppClassLoader@1b26f7b2, sun.misc.Launcher$ExtClassLoader@1a7cb3a4)
22/06/12 15:07:43 INFO SparkServiceRPCServer: Initializing Spark Service RPC Server. Classloader stack: List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@5ec9eefa, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@356fa0d1, sun.misc.Launcher$AppClassLoader@1b26f7b2, sun.misc.Launcher$ExtClassLoader@1a7cb3a4)
22/06/12 15:07:43 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
22/06/12 15:07:43 INFO SparkServiceRPCServer: Starting Spark Service RPC Server. Classloader stack: List(com.databricks.backend.daemon.driver.ClassLoaders$MultiReplClassLoader@5ec9eefa, com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader@356fa0d1, sun.misc.Launcher$AppClassLoader@1b26f7b2, sun.misc.Launcher$ExtClassLoader@1a7cb3a4)
22/06/12 15:07:43 INFO Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_302-b08
22/06/12 15:07:43 INFO AbstractConnector: Started ServerConnector@53fbb2b4{HTTP/1.1, (http/1.1)}{0.0.0.0:15001}
22/06/12 15:07:43 INFO Server: Started @20170ms
22/06/12 15:07:43 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
22/06/12 15:07:43 INFO DatabricksILoop$: Successfully initialized SparkContext
22/06/12 15:07:43 INFO DBFS: Initialized DBFS with DBFSV1 as the delegate.
22/06/12 15:07:43 INFO log: Logging initialized @20617ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog
22/06/12 15:07:44 INFO TypeUtil: JVM Runtime does not support Modules
22/06/12 15:07:45 INFO SharedState: Scheduler stats enabled.
22/06/12 15:07:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
22/06/12 15:07:45 INFO SharedState: Warehouse path is 'dbfs:/user/hive/warehouse'.
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77ec9fdb{/storage/iocache,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2361365c{/storage/iocache/json,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@30b9728f{/SQL,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@453a30f8{/SQL/json,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29ccab93{/SQL/execution,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@24b8a393{/SQL/execution/json,null,AVAILABLE,@Spark}
22/06/12 15:07:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9763e61{/static/sql,null,AVAILABLE,@Spark}
22/06/12 15:07:45 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:45 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:49 INFO HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml
22/06/12 15:07:50 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
22/06/12 15:07:50 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
22/06/12 15:07:50 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
22/06/12 15:07:50 INFO AbstractService: Service:OperationManager is inited.
22/06/12 15:07:50 INFO AbstractService: Service:SessionManager is inited.
22/06/12 15:07:50 INFO SparkSQLCLIService: Service: CLIService is inited.
22/06/12 15:07:50 INFO AbstractService: Service:ThriftHttpCLIService is inited.
22/06/12 15:07:50 INFO HiveThriftServer2: Service: HiveServer2 is inited.
22/06/12 15:07:50 INFO AbstractService: Service:OperationManager is started.
22/06/12 15:07:50 INFO AbstractService: Service:SessionManager is started.
22/06/12 15:07:50 INFO SparkSQLCLIService: Service: CLIService is started.
22/06/12 15:07:50 INFO AbstractService: Service:ThriftHttpCLIService is started.
22/06/12 15:07:50 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
22/06/12 15:07:50 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
22/06/12 15:07:50 INFO Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_302-b08
22/06/12 15:07:50 INFO session: DefaultSessionIdManager workerName=node0
22/06/12 15:07:50 INFO session: No SessionScavenger set, using defaults
22/06/12 15:07:50 INFO session: node0 Scavenging every 660000ms
22/06/12 15:07:50 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@6b8cdc4{/,null,STARTING} has uncovered http methods for path: /*
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b8cdc4{/,null,AVAILABLE}
22/06/12 15:07:50 INFO SslContextFactory: x509=X509@4f8641d1(1,h=[databrickscloud.com],a=[],w=[]) for Server@5cfa9e8a[provider=null,keyStore=file:///databricks/keys/jetty-ssl-driver-keystore.jks,trustStore=null]
22/06/12 15:07:50 INFO AbstractConnector: Started ServerConnector@4cd079fe{SSL, (ssl, http/1.1)}{0.0.0.0:10000}
22/06/12 15:07:50 INFO Server: Started @27008ms
22/06/12 15:07:50 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
22/06/12 15:07:50 INFO AbstractService: Service:HiveServer2 is started.
22/06/12 15:07:50 INFO HiveThriftServer2: HiveThriftServer2 started
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@425a5c2e{/sqlserver,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@44a7661d{/sqlserver/json,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53cb9b1{/sqlserver/session,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61e34bec{/sqlserver/session/json,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO DriverCorral: Creating the driver context
22/06/12 15:07:50 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60e5db1d{/StreamingQuery,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32eac39b{/StreamingQuery/json,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@48f95f96{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@24bc06dd{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5fb0a09e{/static/sql,null,AVAILABLE,@Spark}
22/06/12 15:07:50 INFO DriverDaemon: Starting driver daemon...
22/06/12 15:07:50 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
22/06/12 15:07:50 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
22/06/12 15:07:50 INFO DriverDaemon$: Attempting to run: 'set up ttyd daemon'
22/06/12 15:07:50 INFO DriverDaemon$: Attempting to run: 'Configuring RStudio daemon'
22/06/12 15:07:50 INFO DriverDaemon$: Attempting to run: 'Configuring server to accept connections from client-side tunnels'
22/06/12 15:07:51 INFO Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_302-b08
22/06/12 15:07:51 INFO DriverDaemon$$anon$1: Message out thread ready
22/06/12 15:07:51 INFO AbstractConnector: Started ServerConnector@3c7787a7{HTTP/1.1, (http/1.1)}{0.0.0.0:6061}
22/06/12 15:07:51 INFO Server: Started @27654ms
22/06/12 15:07:51 INFO Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_302-b08
22/06/12 15:07:51 INFO SslContextFactory: x509=X509@1a865ebf(1,h=[databrickscloud.com],a=[],w=[]) for Server@6b3d3e57[provider=null,keyStore=null,trustStore=null]
22/06/12 15:07:51 WARN config: Weak cipher suite TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA enabled for Server@6b3d3e57[provider=null,keyStore=null,trustStore=null]
22/06/12 15:07:51 WARN config: Weak cipher suite TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA enabled for Server@6b3d3e57[provider=null,keyStore=null,trustStore=null]
22/06/12 15:07:51 INFO AbstractConnector: Started ServerConnector@7866ffa{SSL, (ssl, http/1.1)}{0.0.0.0:6062}
22/06/12 15:07:51 INFO Server: Started @27700ms
22/06/12 15:07:51 INFO DriverDaemon: Started comm channel server
22/06/12 15:07:51 INFO DriverDaemon: Driver daemon started.
22/06/12 15:07:51 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
22/06/12 15:07:52 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:52 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:52 INFO DriverCorral: Loading the root classloader
22/06/12 15:07:52 INFO DriverCorral: Starting sql repl ReplId-4931d-c90b4-3a071-f
22/06/12 15:07:52 INFO DriverCorral: Starting sql repl ReplId-53e48-5fc58-b4dde-7
22/06/12 15:07:52 INFO DriverCorral: Starting sql repl ReplId-6d909-3e5e6-a1d97-c
22/06/12 15:07:53 INFO DriverCorral: Starting sql repl ReplId-2c2ac-0ad4b-bb279-1
22/06/12 15:07:53 INFO DriverCorral: Starting sql repl ReplId-407a8-91fb3-47c2e
22/06/12 15:07:53 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:53 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:07:53 INFO DriverCorral: Starting r repl ReplId-3f6c9-1d24f-355b7-2
22/06/12 15:07:53 INFO SQLDriverWrapper: setupRepl:ReplId-407a8-91fb3-47c2e: finished to load
22/06/12 15:07:53 INFO SQLDriverWrapper: setupRepl:ReplId-2c2ac-0ad4b-bb279-1: finished to load
22/06/12 15:07:53 INFO SQLDriverWrapper: setupRepl:ReplId-4931d-c90b4-3a071-f: finished to load
22/06/12 15:07:53 INFO SQLDriverWrapper: setupRepl:ReplId-6d909-3e5e6-a1d97-c: finished to load
22/06/12 15:07:53 INFO SQLDriverWrapper: setupRepl:ReplId-53e48-5fc58-b4dde-7: finished to load
22/06/12 15:07:53 INFO ROutputStreamHandler: Connection succeeded on port 33927
22/06/12 15:07:53 INFO ROutputStreamHandler: Connection succeeded on port 42307
22/06/12 15:07:53 INFO RDriverLocal: 1. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: object created with for ReplId-3f6c9-1d24f-355b7-2.
22/06/12 15:07:53 INFO RDriverLocal: 2. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: initializing ...
22/06/12 15:07:53 INFO RDriverLocal: 3. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: started RBackend thread on port 41465
22/06/12 15:07:53 INFO RDriverLocal: 4. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: waiting for SparkR to be installed ...
22/06/12 15:08:14 INFO RDriverLocal$: SparkR installation completed.
22/06/12 15:08:14 INFO RDriverLocal: 5. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: launching R process ...
22/06/12 15:08:14 INFO RDriverLocal: 6. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: cgroup isolation disabled, not placing R process in REPL cgroup.
22/06/12 15:08:14 INFO RDriverLocal: 7. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: starting R process on port 1100 (attempt 1) ...
22/06/12 15:08:14 INFO RDriverLocal$: Debugging command for R process builder: SIMBASPARKINI=/etc/simba.sparkodbc.ini R_LIBS=/local_disk0/.ephemeral_nfs/envs/rEnv-ef734ce8-7c77-4a0e-a6fb-7238f2ae485f:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r LD_LIBRARY_PATH=/opt/simba/sparkodbc/lib/64/ DB_DISABLE_R_OUTPUT_STREAMING=false SPARKR_BACKEND_CONNECTION_TIMEOUT=604800 DB_STREAM_BEACON_STRING_START=DATABRICKS_STREAM_START-ReplId-3f6c9-1d24f-355b7-2 DB_STDOUT_STREAM_PORT=33927 SPARKR_BACKEND_AUTH_SECRET=b6636d2d92b8e2189f9f742c0c53998625c8b8e7c97de3c9ca3d2bda59d8530c DB_STREAM_BEACON_STRING_END=DATABRICKS_STREAM_END-ReplId-3f6c9-1d24f-355b7-2 EXISTING_SPARKR_BACKEND_PORT=41465 ODBCINI=/etc/odbc.ini DB_STDERR_STREAM_PORT=42307 /bin/bash /local_disk0/tmp/_startR.sh2080879591502084649resource.r /local_disk0/tmp/_rServeScript.r2998143690790638581resource.r 1100 None
22/06/12 15:08:14 INFO RDriverLocal: 8. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: setting up BufferedStreamThread with bufferSize: 1000.
22/06/12 15:08:16 INFO RDriverLocal: 9. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: R process started with RServe listening on port 1100.
22/06/12 15:08:16 INFO RDriverLocal: 10. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: starting interpreter to talk to R process ...
22/06/12 15:08:17 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
22/06/12 15:08:17 INFO ROutputStreamHandler: Successfully connected to stdout in the RShell.
22/06/12 15:08:17 INFO ROutputStreamHandler: Successfully connected to stderr in the RShell.
22/06/12 15:08:17 INFO RDriverLocal: 11. RDriverLocal.1729356b-8526-4835-9e6e-54eb6e722bde: R interpreter is connected.
22/06/12 15:08:17 INFO RDriverWrapper: setupRepl:ReplId-3f6c9-1d24f-355b7-2: finished to load
22/06/12 15:08:45 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:45 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:45 INFO DriverCorral: Starting scala repl ReplId-34708-bc090-bf06b-d
22/06/12 15:08:45 INFO ScalaDriverWrapper: setupRepl:ReplId-34708-bc090-bf06b-d: finished to load
22/06/12 15:08:46 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:46 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:46 INFO DriverCorral: Starting python repl ReplId-3ffaf-1033d-4846b-9
22/06/12 15:08:46 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:46 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:46 INFO DriverCorral: Starting sql repl ReplId-7d193-9c43f-81f0f-e
22/06/12 15:08:46 INFO SQLDriverWrapper: setupRepl:ReplId-7d193-9c43f-81f0f-e: finished to load
22/06/12 15:08:46 INFO JupyterDriverLocal: Starting gateway server for repl ReplId-3ffaf-1033d-4846b-9
22/06/12 15:08:46 INFO PythonPy4JUtil: Using default multithreaded mode in Py4J
22/06/12 15:08:47 INFO Utils: resolved command to be run: List(virtualenv, /local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019, -p, /databricks/python/bin/python, --no-download, --no-setuptools, --no-wheel)
22/06/12 15:08:47 INFO ProgressReporter$: Added result fetcher for 9014299644200612094_5064677207693743462_df98c210-7711-4e76-b4f3-cf602c4d1770
22/06/12 15:08:48 INFO DatabricksUtils: created python virtualenv: /local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019
22/06/12 15:08:48 INFO Utils: resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))
22/06/12 15:08:48 INFO Utils: resolved command to be run: List(/local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019/bin/python, -c, from distutils.sysconfig import get_python_lib; print(get_python_lib()))
22/06/12 15:08:48 INFO DatabricksUtils: created sites.pth at /local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019/lib/python3.8/site-packages/sites.pth
22/06/12 15:08:48 INFO DBUtilsPythonEnvManager: Time spent to start virtualenv /local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019 is 1624
22/06/12 15:08:48 INFO IpykernelUtils$: Python process builder: [/databricks/spark/python/pyspark/wrapped_python.py, root, /local_disk0/pythonVirtualEnvDirs/virtualEnv-b040d585-7103-435d-8db8-f27be6f51019/bin/python, /databricks/python_shell/scripts/db_ipykernel_launcher.py, -f, /tmp/ipykernel-connection-ReplId-3ffaf-1033d-4846b-9.json]
22/06/12 15:08:49 INFO ClusterLoadMonitor: Added query with execution ID:0. Current active queries:1
22/06/12 15:08:49 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
22/06/12 15:08:49 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 
22/06/12 15:08:52 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
22/06/12 15:08:52 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
22/06/12 15:08:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--oro--oro--oro__oro__2.0.8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.servlet--servlet-api--javax.servlet__servlet-api__2.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.mortbay.jetty--servlet-api--org.mortbay.jetty__servlet-api__2.5-20081211.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive.shims--hive-shims-common-secure--org.apache.hive.shims__hive-shims-common-secure__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.google.guava--guava--com.google.guava__guava__11.0.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.eclipse.jetty.aggregate--jetty-all--org.eclipse.jetty.aggregate__jetty-all__7.6.0.v20120127.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.0.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-ant--org.apache.hive__hive-ant__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-exec--org.apache.hive__hive-exec__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.geronimo.specs--geronimo-jta_1.1_spec--org.apache.geronimo.specs__geronimo-jta_1.1_spec__1.1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive.shims--hive-shims-0.20S--org.apache.hive.shims__hive-shims-0.20S__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-common--org.apache.hive__hive-common__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.avro--avro--org.apache.avro__avro__1.7.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.zaxxer--HikariCP--com.zaxxer__HikariCP__2.5.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-service--org.apache.hive__hive-service__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.ow2.asm--asm--org.ow2.asm__asm__4.0.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.mortbay.jetty--jetty-util--org.mortbay.jetty__jetty-util__6.1.26.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.ant--ant--org.apache.ant__ant__1.9.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-cli--org.apache.hive__hive-cli__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.0.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-serde--org.apache.hive__hive-serde__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.2.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/databricks-hive/manifest.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive.shims--hive-shims-0.20--org.apache.hive.shims__hive-shims-0.20__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--asm--asm-tree--asm__asm-tree__3.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--junit--junit--junit__junit__3.8.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.activation--activation--javax.activation__activation__1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.geronimo.specs--geronimo-jaspic_1.0_spec--org.apache.geronimo.specs__geronimo-jaspic_1.0_spec__1.0.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-shims--org.apache.hive__hive-shims__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__1.3.9.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.mail--mail--javax.mail__mail__1.4.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--jline--jline--jline__jline__0.9.94.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--commons-lang--commons-lang--commons-lang__commons-lang__2.4.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--asm--asm-commons--asm__asm-commons__3.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--log4j--log4j--log4j__log4j__1.2.16.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.geronimo.specs--geronimo-annotation_1.0_spec--org.apache.geronimo.specs__geronimo-annotation_1.0_spec__1.1.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--asm--asm--asm__asm__3.1.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.mortbay.jetty--jetty--org.mortbay.jetty__jetty__6.1.26.jar:file:/databricks/databricks-hive/----workspace_spark_3_2--maven-trees--hive-metastore-databricks--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/databricks-hive/bonecp-configs.jar
22/06/12 15:08:52 INFO PoolingHiveClient: Hive metastore connection pool implementation is HikariCP
22/06/12 15:08:52 INFO LocalHiveClientsPool: Create Hive Metastore client pool of size 20
22/06/12 15:08:53 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is dbfs:/user/hive/warehouse
22/06/12 15:08:53 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22/06/12 15:08:54 INFO PythonDriverWrapper: setupRepl:ReplId-3ffaf-1033d-4846b-9: finished to load
22/06/12 15:08:54 INFO ProgressReporter$: Added result fetcher for 4610262164782466745_6604305271602718824_6b26cd3f-7a26-4de7-9733-794f54595c6f
22/06/12 15:08:54 INFO ObjectStore: ObjectStore, initialize called
22/06/12 15:08:54 INFO Persistence: Property datanucleus.fixedDatastore unknown - will be ignored
22/06/12 15:08:54 INFO Persistence: Property datanucleus.connectionPool.idleTimeout unknown - will be ignored
22/06/12 15:08:54 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
22/06/12 15:08:54 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
22/06/12 15:08:54 INFO ProgressReporter$: Removed result fetcher for 4610262164782466745_6604305271602718824_6b26cd3f-7a26-4de7-9733-794f54595c6f
22/06/12 15:08:55 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
22/06/12 15:08:55 INFO HikariDataSource: HikariPool-1 - Started.
22/06/12 15:08:55 WARN DriverDataSource: Registered driver with driverClassName=org.apache.derby.jdbc.EmbeddedDriver was not found, trying direct instantiation.
22/06/12 15:08:56 INFO PoolBase: HikariPool-1 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
22/06/12 15:08:56 INFO HikariDataSource: HikariPool-2 - Started.
22/06/12 15:08:56 WARN DriverDataSource: Registered driver with driverClassName=org.apache.derby.jdbc.EmbeddedDriver was not found, trying direct instantiation.
22/06/12 15:08:56 INFO PoolBase: HikariPool-2 - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)
22/06/12 15:08:57 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
22/06/12 15:08:57 INFO MetaStoreDirectSql: MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22/06/12 15:08:58 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
22/06/12 15:09:01 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
22/06/12 15:09:02 INFO ObjectStore: Initialized ObjectStore
22/06/12 15:09:02 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.0
22/06/12 15:09:02 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 0.13.0, comment = Set by MetaStore root@127.0.1.1
22/06/12 15:09:02 INFO HiveMetaStore: Added admin role in metastore
22/06/12 15:09:02 INFO HiveMetaStore: Added public role in metastore
22/06/12 15:09:02 INFO HiveMetaStore: No user is added in admin role, since config is empty
22/06/12 15:09:03 INFO HiveMetaStore: 0: get_database: default
22/06/12 15:09:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
22/06/12 15:09:03 INFO HiveMetaStore: 0: get_databases: *
22/06/12 15:09:03 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
22/06/12 15:09:03 INFO CodeGenerator: Code generated in 424.306911 ms
22/06/12 15:09:03 INFO ClusterLoadMonitor: Removed query with execution ID:0. Current active queries:0
22/06/12 15:09:03 INFO ClusterLoadMonitor: Added query with execution ID:1. Current active queries:1
22/06/12 15:09:04 INFO ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 
22/06/12 15:09:04 INFO CodeGenerator: Code generated in 39.089778 ms
22/06/12 15:09:04 INFO ClusterLoadMonitor: Removed query with execution ID:1. Current active queries:0
22/06/12 15:09:04 INFO CodeGenerator: Code generated in 83.979012 ms
22/06/12 15:09:04 INFO ProgressReporter$: Removed result fetcher for 9014299644200612094_5064677207693743462_df98c210-7711-4e76-b4f3-cf602c4d1770
22/06/12 15:09:05 INFO ProgressReporter$: Added result fetcher for 9014299644200612094_6151065667942766531_efd62e8f-8002-40b5-a639-ba538521c950
22/06/12 15:09:05 WARN SimpleFunctionRegistry: The function getargument replaced a previously registered function.
22/06/12 15:09:05 INFO ClusterLoadMonitor: Added query with execution ID:2. Current active queries:1
22/06/12 15:09:05 INFO HiveMetaStore: 0: get_database: global_temp
22/06/12 15:09:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
22/06/12 15:09:05 ERROR RetryingHMSHandler: NoSuchObjectException(message:There is no database named global_temp)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:508)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy58.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy60.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:949)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy61.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1165)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1154)
	at org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:619)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:435)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:335)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:236)
	at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:272)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:315)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:435)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$databaseExists$1(PoolingHiveClient.scala:321)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$databaseExists$1$adapted(PoolingHiveClient.scala:320)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:149)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.databaseExists(PoolingHiveClient.scala:320)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:300)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:151)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:112)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:150)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:149)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:300)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.databaseExists(ExternalCatalogWithListener.scala:77)
	at org.apache.spark.sql.internal.SharedState.$anonfun$globalTempViewManager$1(SharedState.scala:234)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:234)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:231)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$hiveCatalog$2(HiveSessionStateBuilder.scala:75)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.globalTempViewManager$lzycompute(SessionCatalog.scala:525)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.globalTempViewManager(SessionCatalog.scala:525)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.globalTempViewManagerName$lzycompute(ManagedCatalogSessionCatalog.scala:98)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.globalTempViewManagerName(ManagedCatalogSessionCatalog.scala:97)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1179)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listTables(ManagedCatalogSessionCatalog.scala:1276)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$listTables$1(UnityCatalogV2Proxy.scala:184)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:114)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listTables(UnityCatalogV2Proxy.scala:183)
	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:42)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:310)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:225)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:101)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:803)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:798)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)
	at scala.collection.immutable.List.map(List.scala:293)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:605)
	at com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)
	at com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:582)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)
	at java.lang.Thread.run(Thread.java:748)

22/06/12 15:09:05 INFO HiveMetaStore: 0: get_database: default
22/06/12 15:09:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
22/06/12 15:09:05 INFO HiveMetaStore: 0: get_database: default
22/06/12 15:09:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
22/06/12 15:09:05 INFO HiveMetaStore: 0: get_tables: db=default pat=*
22/06/12 15:09:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
22/06/12 15:09:05 INFO ClusterLoadMonitor: Removed query with execution ID:2. Current active queries:0
22/06/12 15:09:05 INFO ClusterLoadMonitor: Added query with execution ID:3. Current active queries:1
22/06/12 15:09:05 INFO ClusterLoadMonitor: Removed query with execution ID:3. Current active queries:0
22/06/12 15:09:05 INFO ProgressReporter$: Removed result fetcher for 9014299644200612094_6151065667942766531_efd62e8f-8002-40b5-a639-ba538521c950
22/06/12 15:09:06 INFO AsyncEventQueue: Process of event SparkListenerSQLUsageLogging(executionId=0, ...) by listener SQLAppStatusListener took 2.280694911s.
22/06/12 15:09:07 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 
22/06/12 15:09:10 INFO ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 
22/06/12 15:09:43 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_9207580444752405130_5b34d7640630404387146e8679f7a1f3
22/06/12 15:09:43 INFO SignalUtils: Registering signal handler for INT
22/06/12 15:09:45 INFO DriverILoop: Set class prefix to: $line147f8f407a6e4de2ae7d8e91eda70bf4
22/06/12 15:09:45 INFO DriverILoop: set ContextClassLoader
22/06/12 15:09:45 INFO DriverILoop: initialized intp
22/06/12 15:09:55 INFO ProgressReporter$: Removed result fetcher for 3778673746542266045_9207580444752405130_5b34d7640630404387146e8679f7a1f3
22/06/12 15:12:03 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_6323277215583701840_72328c595a6a4650a129b007467affe9
22/06/12 15:12:03 WARN ScalaDriverLocal: User Code Compile error: command-3221359628625560:3: error: not found: value Level
Logger.getLogger("TUTAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ").setLevel(Level.INFO)
                                                              ^

22/06/12 15:12:03 INFO ProgressReporter$: Removed result fetcher for 3778673746542266045_6323277215583701840_72328c595a6a4650a129b007467affe9
22/06/12 15:12:23 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_4652528813575221502_5341b5ce7d0f4f89bf84affec344c70a
22/06/12 15:12:23 INFO ProgressReporter$: Removed result fetcher for 3778673746542266045_4652528813575221502_5341b5ce7d0f4f89bf84affec344c70a
22/06/12 15:12:50 INFO HiveMetaStore: 1: get_database: default
22/06/12 15:12:50 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
22/06/12 15:12:50 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
22/06/12 15:12:50 INFO ObjectStore: ObjectStore, initialize called
22/06/12 15:12:50 INFO MetaStoreDirectSql: MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: "@" (64), after : "".
22/06/12 15:12:50 INFO ObjectStore: Initialized ObjectStore
22/06/12 15:12:50 INFO DriverCorral: Metastore health check ok
22/06/12 15:12:50 INFO DriverCorral: DBFS health check ok
22/06/12 15:15:28 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_5832715686235275944_10fb6b23ed004ec49e6213e192ef74c7
22/06/12 15:15:28 WARN ScalaDriverLocal: User Code Compile error: command-3221359628625560:2: error: not found: value LogManager
 LogManager.getRootLogger
 ^

22/06/12 15:15:28 INFO ProgressReporter$: Removed result fetcher for 3778673746542266045_5832715686235275944_10fb6b23ed004ec49e6213e192ef74c7
22/06/12 15:15:37 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_8948231597181160276_9e189ebcab784d60b076f452af63ec9c
22/06/12 15:15:37 INFO ProgressReporter$: Removed result fetcher for 3778673746542266045_8948231597181160276_9e189ebcab784d60b076f452af63ec9c
22/06/12 15:16:03 INFO ProgressReporter$: Added result fetcher for 3778673746542266045_8798197498438619411_547113d27da04719a8c738adc9493561
22/06/12 15:16:04 WARN root: TUTAAAAAAAAAAAAAAAAAAAAAAAAAAAJ
